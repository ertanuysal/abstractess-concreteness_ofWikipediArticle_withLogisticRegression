{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from itertools import tee, islice, chain\n",
    "import wikipedia\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVikipedia():\n",
    "    \n",
    "    text=wikipedia.page(\"Superman\").content\n",
    "    text2=wikipedia.page(\"Rocky\").content\n",
    "    text3=wikipedia.page(\"The Piano\").content\n",
    "    text4=wikipedia.page(\"Titanic\").content\n",
    "    text5=wikipedia.page(\"Inception\").content\n",
    "    text6=wikipedia.page(\"Die Hard\").content\n",
    "    text7=wikipedia.page(\"Godfather\").content\n",
    "    text8=wikipedia.page(\"Tokyo Story\").content\n",
    "    \n",
    "    \n",
    "    texts=text+text2+text3+text4+text5+text6+text7+text8\n",
    "    \n",
    "    array=texts.split()\n",
    "\n",
    "    return array\n",
    "\n",
    "def searchWord():\n",
    "    \n",
    "    array=getVikipedia()\n",
    "    dataset=[]\n",
    "    for i in array:\n",
    "        if(compareWithTxt(i)==True):\n",
    "            dataset.append(i)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compareWithTxt(word):\n",
    "    \n",
    "    paths=['/Users/ertanuysal/Downloads/2020-assignment_2 (4) 2/100-400.txt',\n",
    "           '/Users/ertanuysal/Downloads/2020-assignment_2 (4) 2/400-700.txt']\n",
    "    \n",
    "    for i in paths:\n",
    "    \n",
    "        f = open(i)\n",
    "\n",
    "        # use readline() to read the first line \n",
    "        line = f.readline()\n",
    "\n",
    "        while line:\n",
    "\n",
    "            wordsOfLine=line.split()\n",
    "\n",
    "            if(word.upper() == wordsOfLine[0]):\n",
    "                return True\n",
    "\n",
    "            line = f.readline() # use realine() to read next line\n",
    "        f.close()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim of this code block is getting data as word form in wikipedi and checking whether it is in txt files or not.**\n",
    "\n",
    "<h3>getVikipedia:</h3> Get vikipedi articles context with word form\n",
    "    \n",
    "<h3>searchWord:</h3> If the word is in the txt files ,add into dataset list.\n",
    "    \n",
    "<h3>compareWithTxt:</h3> Check whether word is in the txt files or not.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def  checkStrongOrWeak(word):\n",
    "\n",
    "    f = open('/Users/ertanuysal/Desktop/ML_HW2_250201064/subjclueslen1-HLTEMNLP05.txt')\n",
    "    # use readline() to read the first line \n",
    "    line = f.readline()\n",
    "\n",
    "    while line:\n",
    "\n",
    "        \n",
    "        wordsOfLine=line.split()\n",
    "        nameOfLine=wordsOfLine[2].split(\"=\")\n",
    "        \n",
    "        if(nameOfLine[1]==word):\n",
    "            types=wordsOfLine[0].split(\"=\")\n",
    "            \n",
    "            if(types[1]==\"strongsubj\"):\n",
    "                return True\n",
    "            else:   \n",
    "                return False\n",
    "              \n",
    "        line = f.readline() # use realine() to read next line\n",
    "    f.close()\n",
    "\n",
    "def get_pos(string):\n",
    "    string = nltk.word_tokenize(string)\n",
    "    pos_string = nltk.pos_tag(string)\n",
    "    return pos_string [0][1]\n",
    "\n",
    "\n",
    "def pozOrNeg(word):\n",
    "    \n",
    "    sent          = TextBlob(word)\n",
    "    sent          = TextBlob(word, analyzer = NaiveBayesAnalyzer())\n",
    "    classification= sent.sentiment.classification\n",
    "    if (classification==\"pos\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def score(word):\n",
    "    \n",
    "    good = swn.senti_synsets(word, 'n')\n",
    "    posscore=0\n",
    "    negscore=0\n",
    "    for synst in good:\n",
    "\n",
    "        posscore=posscore+synst.pos_score()\n",
    "        negscore=negscore+synst.neg_score()\n",
    "    if(posscore>negscore):\n",
    "        return True\n",
    "    if(negscore>posscore):\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def findPoint(word):\n",
    "    paths=['/Users/ertanuysal/Downloads/2020-assignment_2 (4) 2/100-400.txt',\n",
    "           '/Users/ertanuysal/Downloads/2020-assignment_2 (4) 2/400-700.txt']\n",
    "    \n",
    "    for i in paths:\n",
    "        f = open(i)\n",
    "\n",
    "        # use readline() to read the first line \n",
    "        line = f.readline()\n",
    "\n",
    "        while line:\n",
    "            wordsOfLine=line.split()\n",
    "            if(wordsOfLine[0]==word.upper()):\n",
    "                \n",
    "                score=wordsOfLine[1]\n",
    "                score=int(score)\n",
    "                print(score)\n",
    "\n",
    "                if(score>323):\n",
    "                    return True\n",
    "\n",
    "            line = f.readline() # use realine() to read next line\n",
    "        f.close()\n",
    "    return False\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim of this code block is create for features for each word.**\n",
    "\n",
    "<h3>checkStrongOrWeak:</h3> Check the word is strongword or weakword according to subjclueslen1-HLTEMNLP05.txt file.\n",
    "\n",
    "<h3>get_pos:</h3> Get the part of speach tag of word\n",
    "\n",
    "<h3>pozOrNeg:</h3> Check the word is pozitif or negatif word.\n",
    "\n",
    "<h3>score:</h3> Check the word is pozitivie or negative word.(i used this method in this assinghment because it is faster)\n",
    "\n",
    "<h3>findPoint:</h3> Defines the label of word as abstract or concrete with threshold(we determined threshold value as 323 to ensure a balanced distribution for labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "his\n",
      "love\n",
      "interest\n",
      "and\n",
      "311\n",
      "over\n",
      "their\n",
      "love\n",
      "of\n",
      "to\n",
      "311\n",
      "is\n",
      "his\n",
      "will\n",
      "by\n",
      "an\n",
      "275\n",
      "was\n",
      "a\n",
      "hero\n",
      "of\n",
      "a\n",
      "428\n",
      "and\n",
      "the\n",
      "opportunity\n",
      "to\n",
      "present\n",
      "306\n",
      "that\n",
      "he\n",
      "must\n",
      "his\n",
      "for\n",
      "297\n",
      "to\n",
      "a\n",
      "star\n",
      "football\n",
      "The\n",
      "574\n",
      "not\n",
      "two\n",
      "worth\n",
      "of\n",
      "on\n",
      "257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a6abe9da2061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#print(searchWord())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#print(getVikipedia())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a6abe9da2061>\u001b[0m in \u001b[0;36mmainFunction\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtwoAfter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mcheckStrong\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckStrongOrWeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mcheckWeak\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckStrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpozitivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e495c213b2b>\u001b[0m in \u001b[0;36mcheckStrongOrWeak\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use realine() to read next line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rows=[]       \n",
    "def mainFunction(dataset):\n",
    "    \n",
    "    twoBefore,previous,next_,twoAfter,current= None,None,None,None,None\n",
    "    l = len(dataset)\n",
    "    \n",
    "\n",
    "    for index, obj in enumerate(dataset):\n",
    "        current=dataset[index]\n",
    "        if index > 0:\n",
    "            previous = dataset[index - 1]\n",
    "        if index < (l - 1):\n",
    "            next_ = dataset[index + 1]\n",
    "\n",
    "        if index > 0:\n",
    "            twoBefore = dataset[index - 2]\n",
    "        if index < (l - 2):\n",
    "            twoAfter = dataset[index + 2]\n",
    "\n",
    "        checkStrong=checkStrongOrWeak(current)\n",
    "        checkWeak= not checkStrong\n",
    "        pozitivity=score(current)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(twoBefore and previous and next_ and twoAfter and current and checkStrong and pozitivity != None):\n",
    "        \n",
    "            print(twoBefore)\n",
    "            print(previous)\n",
    "            print(current)\n",
    "            print(next_)\n",
    "            print(twoAfter)\n",
    "\n",
    "            pos=get_pos(current)\n",
    "            pos_before=get_pos(previous)\n",
    "            pos_2before=get_pos(twoBefore)\n",
    "            pos_after=get_pos(next_)\n",
    "            pos_2after=get_pos(twoAfter)\n",
    "\n",
    "            \n",
    "            pozitivity=score(current)\n",
    "            negativity= not pozitivity\n",
    "            \n",
    "            pozitivity_after=score(next_)\n",
    "\n",
    "            \n",
    "            pozitivity_before=score(previous)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if(pozitivity_before==None):\n",
    "                negativity_before=None\n",
    "            else:  \n",
    "                negativity_before=not pozitivity_before\n",
    "                \n",
    "            point=findPoint(current)\n",
    "\n",
    "        \n",
    "\n",
    "            row=[current ,str(pos) , str(pos_before), str(pos_2before) ,str(pos_after), str(pos_2after) , str(checkStrong) ,str(checkWeak) , str(pozitivity),str(negativity) , str(pozitivity_after), str(negativity_before) ,str(point)]\n",
    "            rows.append(row)\n",
    "            \n",
    "    print(rows)\n",
    "    \n",
    "        #with open('/Users/ertanuysal/Desktop/ML_HW1_MATERIAL/test.csv', 'w') as f:\n",
    "            #for line in rows:\n",
    "                #f.write(line)\n",
    "                #f.write(\"\\n\")\n",
    "            #f.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#test=[\"champion\",\"hate\",\"door\",\"test\",\"snake\",\"no\",\"nine\",\"knisd\",\"yes\",\"ring\",\"wrong\",\"happy\",\"dsad\"]           \n",
    "            \n",
    "\n",
    "            \n",
    "#print(searchWord())\n",
    "print(mainFunction(searchWord()))\n",
    "\n",
    "#print(getVikipedia())   \n",
    "#print(compareWithTxt())\n",
    "#print(checkStrongOrWeak())\n",
    "#print(get_pos())\n",
    "#print(score(\"other\"))\n",
    "#print(pozOrNeg(\"Web\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this block of code is to create a row-by-row dataset by determining the properties of all words.\n",
    "\n",
    "If words do not have pozitivity value , pos tag value with the nearest words and we can not check whether it is strong or not, we removed word in the dataset.(postag,pozitivity-negativity,strongOrWeak features are required)\n",
    "\n",
    "**This code block takes about 6 minutes to run, as it is examined whether each word provides the condition.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "columns = ['word','pos','pos_before','pos_2before','pos_after','pos_2after','checkStrong','checkWeak','pozitivity','negativity','pozitivity_after','negativity_before','label']\n",
    "# load dataset\n",
    "data=pd.DataFrame(rows,columns=columns)\n",
    "#data = pd.read_csv(\"/Users/ertanuysal/Desktop/ML_HW1_MATERIAL/test.csv\", header=None, names=col_names)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This block of code shows part of the dataset with the column names.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].dtype == type(object) or data[column].dtype == type(bool) :\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type conversion is made to run logistic regression function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['word','pos', 'pos_before', 'pos_2before','pos_after','pos_2after','checkStrong','checkWeak','pozitivity','negativity',\n",
    "                'pozitivity_after','negativity_before']\n",
    "X = data[features] # Features\n",
    "y = data.label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features to be used in logistic regression and target variable are determined**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In the dataset,There are 303 words which satisfy the conditions.We allocated %70 of 303 words in train data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train) # training part\n",
    "y_pred=logreg.predict(X_test) #prediction part on test values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAcc(y_pred,y_test):\n",
    "    correct=0\n",
    "    for index,item in enumerate(y_pred):\n",
    "    \n",
    "        if(item==y_test.to_numpy()[index]):\n",
    "            \n",
    "            correct+=1\n",
    "    acc=correct/len(y_test)\n",
    "    return acc\n",
    "calculateAcc(y_pred,y_test)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>calculateAcc:</h3> This method compare each values of y_pred and y_test.If values are same,It means that we predict correctly for that value and increases counter as one.\n",
    "\n",
    "As a result we can obtain accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary confustion matrix of test values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
